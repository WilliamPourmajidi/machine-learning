{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534e0f1d",
   "metadata": {},
   "source": [
    "# Exercise: Comparing Classifiers Using ROC Curves and AUC\n",
    "\n",
    "**Objective:**  \n",
    "In this exercise, you will practice evaluating and comparing different classification models using ROC curves and AUC scores. You will:\n",
    "1. Load and explore a dataset\n",
    "2. Train **two different classifiers** of your choice\n",
    "3. Generate ROC curves for both models\n",
    "4. Calculate AUC scores\n",
    "5. Compare the models and determine which one performs better\n",
    "\n",
    "**Available Classifiers:**\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "\n",
    "**What You'll Learn:**\n",
    "- How to train multiple models on the same dataset\n",
    "- How to generate and interpret ROC curves\n",
    "- How to use AUC to compare model performance\n",
    "- How to make informed decisions about which model is better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485fffb",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Import Libraries\n",
    "\n",
    "First, import all the necessary libraries you'll need for this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a24149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import classifiers (you'll use two of these)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Import metrics for evaluation\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    roc_curve, \n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e9497",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Load and Explore the Dataset\n",
    "\n",
    "We'll use the **Breast Cancer Wisconsin dataset**, which is a binary classification problem. The task is to predict whether a tumor is **malignant (cancerous)** or **benign (non-cancerous)** based on various features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b173de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(pd.Series(y).value_counts())\n",
    "print(\"\\n0 = Malignant (cancerous)\")\n",
    "print(\"1 = Benign (non-cancerous)\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fde5ad",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Train-Test Split\n",
    "\n",
    "Split the data into training and testing sets to evaluate model performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986cbe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data: 75% training, 25% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ebfee",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Train Your First Classifier\n",
    "\n",
    "**TODO:** Choose your first classifier from the options below and train it on the training data.\n",
    "\n",
    "**Options:**\n",
    "- `LogisticRegression(max_iter=10000, random_state=42)`\n",
    "- `DecisionTreeClassifier(random_state=42, max_depth=5)`\n",
    "- `RandomForestClassifier(random_state=42, n_estimators=100)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617de3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize your first classifier\n",
    "# Example: model_1 = LogisticRegression(max_iter=10000, random_state=42)\n",
    "model_1 = None  # Replace None with your chosen classifier\n",
    "\n",
    "# TODO: Fit the model on the training data\n",
    "# model_1.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Make predictions on the test set\n",
    "# y_pred_1 = model_1.predict(X_test)\n",
    "# y_prob_1 = model_1.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "print(\"Model 1 trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743588bb",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Train Your Second Classifier\n",
    "\n",
    "**TODO:** Choose a **different** classifier and train it on the same training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa76866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize your second classifier (choose a DIFFERENT one from model_1)\n",
    "# Example: model_2 = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "model_2 = None  # Replace None with your chosen classifier\n",
    "\n",
    "# TODO: Fit the model on the training data\n",
    "# model_2.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Make predictions on the test set\n",
    "# y_pred_2 = model_2.predict(X_test)\n",
    "# y_prob_2 = model_2.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Model 2 trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003f027b",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6: Evaluate Model 1\n",
    "\n",
    "**TODO:** Calculate key metrics for your first model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91902613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate metrics for Model 1\n",
    "# acc_1 = accuracy_score(y_test, y_pred_1)\n",
    "# prec_1 = precision_score(y_test, y_pred_1)\n",
    "# rec_1 = recall_score(y_test, y_pred_1)\n",
    "# f1_1 = f1_score(y_test, y_pred_1)\n",
    "# auc_1 = roc_auc_score(y_test, y_prob_1)\n",
    "\n",
    "# TODO: Print the metrics\n",
    "# print(\"Model 1 Performance:\")\n",
    "# print(f\"Accuracy: {acc_1:.3f}\")\n",
    "# print(f\"Precision: {prec_1:.3f}\")\n",
    "# print(f\"Recall: {rec_1:.3f}\")\n",
    "# print(f\"F1 Score: {f1_1:.3f}\")\n",
    "# print(f\"AUC: {auc_1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da5b25d",
   "metadata": {},
   "source": [
    "\n",
    "## Step 7: Evaluate Model 2\n",
    "\n",
    "**TODO:** Calculate key metrics for your second model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate metrics for Model 2\n",
    "# acc_2 = accuracy_score(y_test, y_pred_2)\n",
    "# prec_2 = precision_score(y_test, y_pred_2)\n",
    "# rec_2 = recall_score(y_test, y_pred_2)\n",
    "# f1_2 = f1_score(y_test, y_pred_2)\n",
    "# auc_2 = roc_auc_score(y_test, y_prob_2)\n",
    "\n",
    "# TODO: Print the metrics\n",
    "# print(\"Model 2 Performance:\")\n",
    "# print(f\"Accuracy: {acc_2:.3f}\")\n",
    "# print(f\"Precision: {prec_2:.3f}\")\n",
    "# print(f\"Recall: {rec_2:.3f}\")\n",
    "# print(f\"F1 Score: {f1_2:.3f}\")\n",
    "# print(f\"AUC: {auc_2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555e3e6",
   "metadata": {},
   "source": [
    "\n",
    "## Step 8: Plot ROC Curves for Both Models\n",
    "\n",
    "**TODO:** Generate ROC curves for both models on the same plot to visually compare their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate ROC curve points for Model 1\n",
    "# fpr_1, tpr_1, _ = roc_curve(y_test, y_prob_1)\n",
    "\n",
    "# TODO: Calculate ROC curve points for Model 2\n",
    "# fpr_2, tpr_2, _ = roc_curve(y_test, y_prob_2)\n",
    "\n",
    "# TODO: Create the plot\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(fpr_1, tpr_1, label=f'Model 1 (AUC = {auc_1:.2f})', linewidth=2)\n",
    "# plt.plot(fpr_2, tpr_2, label=f'Model 2 (AUC = {auc_2:.2f})', linewidth=2)\n",
    "# plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing (AUC = 0.50)')\n",
    "# plt.xlabel('False Positive Rate', fontsize=12)\n",
    "# plt.ylabel('True Positive Rate', fontsize=12)\n",
    "# plt.title('ROC Curve Comparison', fontsize=14)\n",
    "# plt.legend(fontsize=10)\n",
    "# plt.grid(alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe7b33d",
   "metadata": {},
   "source": [
    "\n",
    "## Step 9: Compare and Interpret\n",
    "\n",
    "**TODO:** Answer the following questions based on your results:\n",
    "\n",
    "1. **Which model has a higher AUC score?**\n",
    "   - Write your answer here:\n",
    "\n",
    "2. **Looking at the ROC curves, which curve is closer to the upper-left corner?**\n",
    "   - Write your answer here:\n",
    "\n",
    "3. **Based on the AUC interpretation guide from the lecture:**\n",
    "   - Model 1 AUC = _____ → Performance level: _____\n",
    "   - Model 2 AUC = _____ → Performance level: _____\n",
    "\n",
    "4. **Which model would you recommend for this task and why?**\n",
    "   - Write your answer here:\n",
    "\n",
    "5. **Are there any trade-offs between the two models?** (Consider accuracy, precision, recall, and interpretability)\n",
    "   - Write your answer here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee337e69",
   "metadata": {},
   "source": [
    "\n",
    "## Step 10: Create a Summary Table\n",
    "\n",
    "**TODO:** Create a comparison table showing all metrics for both models side by side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d54d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a summary DataFrame\n",
    "# summary = pd.DataFrame({\n",
    "#     'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC'],\n",
    "#     'Model 1': [acc_1, prec_1, rec_1, f1_1, auc_1],\n",
    "#     'Model 2': [acc_2, prec_2, rec_2, f1_2, auc_2]\n",
    "# })\n",
    "\n",
    "# TODO: Print the table\n",
    "# print(\"\\nModel Comparison Summary:\")\n",
    "# print(summary.round(3).to_string(index=False))\n",
    "\n",
    "# TODO: Highlight which model wins on each metric\n",
    "# print(\"\\nBest performing model per metric:\")\n",
    "# for idx, row in summary.iterrows():\n",
    "#     metric = row['Metric']\n",
    "#     if row['Model 1'] > row['Model 2']:\n",
    "#         print(f\"{metric}: Model 1 ({row['Model 1']:.3f})\")\n",
    "#     elif row['Model 2'] > row['Model 1']:\n",
    "#         print(f\"{metric}: Model 2 ({row['Model 2']:.3f})\")\n",
    "#     else:\n",
    "#         print(f\"{metric}: Tie ({row['Model 1']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a79dbe",
   "metadata": {},
   "source": [
    "\n",
    "## Bonus Challenge (Optional)\n",
    "\n",
    "**Try the following to deepen your understanding:**\n",
    "\n",
    "1. **Train a third classifier** and add it to the ROC curve comparison. Which of the three performs best?\n",
    "\n",
    "2. **Experiment with hyperparameters:**\n",
    "   - For Decision Tree: try different values of `max_depth` (3, 5, 10, None)\n",
    "   - For Random Forest: try different values of `n_estimators` (50, 100, 200)\n",
    "   - For Logistic Regression: try adding `C=0.1` or `C=10` (regularization strength)\n",
    "   - How does this affect the AUC?\n",
    "\n",
    "3. **Create confusion matrices** for both models and compare the distribution of errors. Which model makes more false positives? Which makes more false negatives?\n",
    "\n",
    "4. **Threshold analysis:** Pick one model and experiment with different decision thresholds (0.3, 0.5, 0.7) to see how precision and recall change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23079b4",
   "metadata": {},
   "source": [
    "\n",
    "## Key Takeaways\n",
    "\n",
    "After completing this exercise, you should understand:\n",
    "\n",
    "1. **ROC curves visualize classifier performance** across all thresholds, not just one cutoff point.\n",
    "\n",
    "2. **AUC provides a single number** to compare models—higher is always better.\n",
    "\n",
    "3. **Different models have different strengths:**\n",
    "   - Logistic Regression: Simple, interpretable, works well with linear relationships\n",
    "   - Decision Tree: Captures non-linear patterns, easy to visualize, but can overfit\n",
    "   - Random Forest: Often high performance, reduces overfitting, but less interpretable\n",
    "\n",
    "4. **The \"best\" model depends on context:** Consider not just AUC but also precision, recall, interpretability, and computational cost.\n",
    "\n",
    "5. **ROC curves help you make informed decisions** about which model to deploy in real-world applications.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
