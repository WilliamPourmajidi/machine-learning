{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a916ad",
   "metadata": {},
   "source": [
    "\n",
    "# Module 5 — In-Class Activity (Starter)\n",
    "**Topic:** Ensemble Learning in Practice  \n",
    "**Time:** ~45–60 minutes\n",
    "\n",
    "You will:\n",
    "- Generate a small, human-readable dataset.\n",
    "- Split and scale data.\n",
    "- Train **baseline** models (Logistic Regression, Decision Tree, Random Forest).\n",
    "- Train **ensemble** models (Bagging and AdaBoost).\n",
    "- Build a **comparison table** (Accuracy, F1) and a quick bar chart.\n",
    "\n",
    "> This is a **starter** notebook: several spots are marked with `# TODO:`.  \n",
    "> Fill them in with Python code before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f1a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# Step 1. Imports and setup\n",
    "# ============================================\n",
    "\n",
    "# TODO: Import any extra packages you need as you go\n",
    "# (You can add them here as you discover missing names.)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(7)  # For reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a899a425",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2. Generate a simple dataset\n",
    "\n",
    "Dataset: **Project Habits → High Grade**  \n",
    "Each row represents a student. Your goal is to predict whether they achieve a high final project grade (≥ 85).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3207d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# Step 2. Generate a simple dataset\n",
    "# ============================================\n",
    "\n",
    "n = 240  # number of samples (students)\n",
    "\n",
    "# randint(low, high, size) draws integers in [low, high)\n",
    "# Make sure the ranges are sensible and easy to explain.\n",
    "DraftsSubmitted    = np.random.randint(0, 6,  n)   # 0..5 drafts (min=0, max=5)\n",
    "PeerReviewsGiven   = np.random.randint(0, 11, n)   # 0..10 peer reviews\n",
    "MeetingsWithTA     = np.random.randint(0, 7,  n)   # 0..6 TA meetings\n",
    "OnTimeSubmissions  = np.random.randint(0, 11, n)   # 0..10 on-time submissions\n",
    "WeekendCodingHours = np.random.randint(0, 16, n)   # 0..15 hrs/weekend\n",
    "\n",
    "# Combine into a dataframe\n",
    "df = pd.DataFrame({\n",
    "    \"DraftsSubmitted\": DraftsSubmitted,\n",
    "    \"PeerReviewsGiven\": PeerReviewsGiven,\n",
    "    \"MeetingsWithTA\": MeetingsWithTA,\n",
    "    \"OnTimeSubmissions\": OnTimeSubmissions,\n",
    "    \"WeekendCodingHours\": WeekendCodingHours\n",
    "})\n",
    "\n",
    "# TODO: Create a rule-based label column \"HighGrade\".\n",
    "# Suggested approach:\n",
    "#   1) Build a 'score' that increases with good practices.\n",
    "#   2) Map score → probability with a logistic function.\n",
    "#   3) Convert to label: HighGrade = 1 if prob > threshold else 0.\n",
    "\n",
    "score = (\n",
    "    1.0 * DraftsSubmitted\n",
    "  + 0.6 * (PeerReviewsGiven / 2.0)\n",
    "  + 0.9 * MeetingsWithTA\n",
    "  + 0.8 * (OnTimeSubmissions / 2.0)\n",
    "  + 0.3 * (WeekendCodingHours / 3.0)\n",
    ")\n",
    "\n",
    "prob = 1.0 / (1.0 + np.exp(-0.9 * (score - 6.5)))\n",
    "df[\"HighGrade\"] = (prob > 0.55).astype(int)\n",
    "\n",
    "# Quick data check\n",
    "display(df.head())\n",
    "print(\"HighGrade rate:\", df[\"HighGrade\"].mean().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ed33cf",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3. Split and scale the data\n",
    "\n",
    "- Use `train_test_split` with `stratify=y` and `test_size=0.25`.\n",
    "- Standardize features for models that need it (e.g., Logistic Regression) using `StandardScaler`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# Step 3. Split and scale the data\n",
    "# ============================================\n",
    "\n",
    "X = df.drop(columns=[\"HighGrade\"])\n",
    "y = df[\"HighGrade\"]\n",
    "\n",
    "# Train/test split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=7, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize for models that benefit from scaling\n",
    "scaler = StandardScaler()\n",
    "X_tr_sc = scaler.fit_transform(X_tr)\n",
    "X_te_sc = scaler.transform(X_te)\n",
    "\n",
    "len(X_tr), len(X_te), y.mean().round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886d8cc6",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4. Train baseline models\n",
    "\n",
    "Train three baselines:\n",
    "- **Logistic Regression** (use **scaled** features)\n",
    "- **Decision Tree** (unscaled)\n",
    "- **Random Forest** (unscaled)\n",
    "\n",
    "Store **Accuracy** and **F1** in a results dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba3b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# Step 4. Train baseline models\n",
    "# ============================================\n",
    "\n",
    "results = {}\n",
    "\n",
    "# TODO: Initialize and fit Logistic Regression on *scaled* data\n",
    "lr = LogisticRegression(max_iter=500, random_state=7)\n",
    "lr.fit(X_tr_sc, y_tr)\n",
    "pred_lr = lr.predict(X_te_sc)\n",
    "results[\"Logistic Regression\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_te, pred_lr),\n",
    "    \"F1\": f1_score(y_te, pred_lr)\n",
    "}\n",
    "\n",
    "# TODO: Initialize and fit Decision Tree on *unscaled* data\n",
    "tree = DecisionTreeClassifier(max_depth=5, random_state=7)\n",
    "tree.fit(X_tr, y_tr)\n",
    "pred_tree = tree.predict(X_te)\n",
    "results[\"Decision Tree\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_te, pred_tree),\n",
    "    \"F1\": f1_score(y_te, pred_tree)\n",
    "}\n",
    "\n",
    "# TODO: Initialize and fit Random Forest on *unscaled* data\n",
    "rf = RandomForestClassifier(n_estimators=250, random_state=7)\n",
    "rf.fit(X_tr, y_tr)\n",
    "pred_rf = rf.predict(X_te)\n",
    "results[\"Random Forest\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_te, pred_rf),\n",
    "    \"F1\": f1_score(y_te, pred_rf)\n",
    "}\n",
    "\n",
    "pd.DataFrame(results).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88105c60",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5. Train ensemble models\n",
    "\n",
    "Implement two ensembles:\n",
    "- **Bagging (Tree)** — variance reduction by averaging many trees.\n",
    "- **AdaBoost** — sequentially fixes mistakes using shallow trees.\n",
    "\n",
    "Add their metrics to the same results dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# Step 5. Train ensemble models\n",
    "# ============================================\n",
    "\n",
    "# TODO: Bagging with DecisionTreeClassifier as base estimator\n",
    "bag = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=5, random_state=7),\n",
    "    n_estimators=200,\n",
    "    random_state=7\n",
    ")\n",
    "bag.fit(X_tr, y_tr)\n",
    "pred_bag = bag.predict(X_te)\n",
    "results[\"Bagging (Tree)\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_te, pred_bag),\n",
    "    \"F1\": f1_score(y_te, pred_bag)\n",
    "}\n",
    "\n",
    "# TODO: AdaBoost with shallow trees (max_depth=2)\n",
    "ada = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=2, random_state=7),\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.5,\n",
    "    random_state=7\n",
    ")\n",
    "ada.fit(X_tr, y_tr)\n",
    "pred_ada = ada.predict(X_te)\n",
    "results[\"AdaBoost\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_te, pred_ada),\n",
    "    \"F1\": f1_score(y_te, pred_ada)\n",
    "}\n",
    "\n",
    "pd.DataFrame(results).T.sort_values(\"F1\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b94c42",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6. Visualize comparison\n",
    "\n",
    "Create a quick bar chart of F1 scores to compare models.\n",
    "Then, tweak **two hyperparameters** (e.g., `n_estimators`, `max_depth`, `learning_rate`) and re-run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc9a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# Step 6. Visualize comparison\n",
    "# ============================================\n",
    "\n",
    "comparison = pd.DataFrame(results).T.sort_values(\"F1\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.barh(comparison.index, comparison[\"F1\"], alpha=0.85, color=\"teal\")\n",
    "plt.xlabel(\"F1 Score\")\n",
    "plt.title(\"Model Comparison: Project Habits Dataset\")\n",
    "plt.grid(axis=\"x\", alpha=0.35)\n",
    "plt.show()\n",
    "\n",
    "comparison\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
