{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a916ad",
   "metadata": {},
   "source": [
    "\n",
    "# Module 5 — In-Class Activity (Starter)\n",
    "**Topic:** Ensemble Learning in Practice  \n",
    "**Time:** ~45–60 minutes\n",
    "\n",
    "You will:\n",
    "- Generate a small, human-readable dataset.\n",
    "- Split and scale data.\n",
    "- Train **baseline** models (Logistic Regression, Decision Tree, Random Forest).\n",
    "- Train **ensemble** models (Bagging and AdaBoost).\n",
    "- Build a **comparison table** (Accuracy, F1) and a quick bar chart.\n",
    "\n",
    "> This is a **starter** notebook: several spots are marked with `# TODO:`.  \n",
    "> Fill them in with Python code before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f1a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Imports and Setup (pre-filled)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(7)  # reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a899a425",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2. Generate a simple dataset\n",
    "\n",
    "Dataset: **Project Habits → High Grade**  \n",
    "Each row represents a student. Your goal is to predict whether they achieve a high final project grade (≥ 85).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3207d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1) Create 5+ integer features with np.random.randint(...)\n",
    "# 2) Assemble a DataFrame df = pd.DataFrame({...})\n",
    "# 3) Build a 'score' that increases with good habits\n",
    "# 4) Map score → probability (logistic or threshold)\n",
    "# 5) df[\"HighGrade\"] = (prob > THRESHOLD).astype(int)\n",
    "# 6) Quick checks: df.head(), class balance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ed33cf",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3. Split and scale the data\n",
    "\n",
    "- Use `train_test_split` with `stratify=y` and `test_size=0.25`.\n",
    "- Standardize features for models that need it (e.g., Logistic Regression) using `StandardScaler`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1) X = df.drop(columns=[\"HighGrade\"]); y = df[\"HighGrade\"]\n",
    "# 2) train_test_split(..., stratify=y, test_size=0.25, random_state=7)\n",
    "# 3) (Optional) StandardScaler: fit on train, transform train & test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886d8cc6",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4. Train baseline models\n",
    "\n",
    "Train three baselines:\n",
    "- **Logistic Regression** (use **scaled** features)\n",
    "- **Decision Tree** (unscaled)\n",
    "- **Random Forest** (unscaled)\n",
    "\n",
    "Store **Accuracy** and **F1** in a results dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba3b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1) Initialize, fit, predict for each baseline\n",
    "# 2) Compute Accuracy, F1 with sklearn.metrics\n",
    "# 3) Populate results dict\n",
    "# 4) Display a table (pd.DataFrame(results).T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88105c60",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5. Train ensemble models\n",
    "\n",
    "Implement two ensembles:\n",
    "- **Bagging (Tree)** — variance reduction by averaging many trees.\n",
    "- **AdaBoost** — sequentially fixes mistakes using shallow trees.\n",
    "\n",
    "Add their metrics to the same results dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1) Fit Bagging with DecisionTree base learner\n",
    "# 2) Fit AdaBoost with shallow trees (e.g., max_depth=2)\n",
    "# 3) Predict, compute metrics, update results\n",
    "# 4) Show updated table sorted by F1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b94c42",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6. Visualize comparison\n",
    "\n",
    "Create a quick bar chart of F1 scores to compare models.\n",
    "Then, tweak **two hyperparameters** (e.g., `n_estimators`, `max_depth`, `learning_rate`) and re-run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc9a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1) comparison = pd.DataFrame(results).T.sort_values(\"F1\", ascending=False)\n",
    "# 2) display(comparison)\n",
    "# 3) Optional plot: barh of F1 by model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PA-Venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
