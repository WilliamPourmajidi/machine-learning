{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4494f0",
   "metadata": {},
   "source": [
    "\n",
    "# Module 5 — Ensemble Methods (Hands-on)\n",
    "\n",
    "In this notebook, we’ll predict whether a student passes or fails based on lifestyle and study habits.  \n",
    "We’ll compare single models to ensemble models and see how combining models improves accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12ee05a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "StudyHours",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "Attendance",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "Assignments",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "SleepHours",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Pass",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "4b33223d-3bc1-4149-9953-9193d1d19d12",
       "rows": [
        [
         "0",
         "7",
         "61",
         "65",
         "6.3",
         "0"
        ],
        [
         "1",
         "4",
         "50",
         "74",
         "6.6",
         "0"
        ],
        [
         "2",
         "8",
         "87",
         "89",
         "8.1",
         "0"
        ],
        [
         "3",
         "5",
         "55",
         "64",
         "8.7",
         "0"
        ],
        [
         "4",
         "7",
         "72",
         "63",
         "7.4",
         "0"
        ],
        [
         "5",
         "10",
         "48",
         "52",
         "7.7",
         "0"
        ],
        [
         "6",
         "3",
         "99",
         "99",
         "6.2",
         "0"
        ],
        [
         "7",
         "7",
         "98",
         "97",
         "8.1",
         "0"
        ],
        [
         "8",
         "8",
         "45",
         "46",
         "7.7",
         "0"
        ],
        [
         "9",
         "5",
         "55",
         "96",
         "6.1",
         "0"
        ],
        [
         "10",
         "4",
         "68",
         "75",
         "4.9",
         "0"
        ],
        [
         "11",
         "8",
         "42",
         "84",
         "5.5",
         "0"
        ],
        [
         "12",
         "8",
         "59",
         "59",
         "6.2",
         "0"
        ],
        [
         "13",
         "3",
         "99",
         "40",
         "8.4",
         "0"
        ],
        [
         "14",
         "6",
         "98",
         "47",
         "8.9",
         "0"
        ],
        [
         "15",
         "5",
         "75",
         "85",
         "6.5",
         "0"
        ],
        [
         "16",
         "2",
         "58",
         "55",
         "6.5",
         "0"
        ],
        [
         "17",
         "8",
         "65",
         "53",
         "7.3",
         "0"
        ],
        [
         "18",
         "6",
         "42",
         "51",
         "7.0",
         "0"
        ],
        [
         "19",
         "2",
         "58",
         "90",
         "8.1",
         "0"
        ],
        [
         "20",
         "5",
         "59",
         "62",
         "6.4",
         "0"
        ],
        [
         "21",
         "1",
         "71",
         "54",
         "8.1",
         "0"
        ],
        [
         "22",
         "10",
         "46",
         "67",
         "6.7",
         "0"
        ],
        [
         "23",
         "6",
         "91",
         "73",
         "8.0",
         "0"
        ],
        [
         "24",
         "9",
         "80",
         "41",
         "6.7",
         "0"
        ],
        [
         "25",
         "1",
         "72",
         "71",
         "6.3",
         "0"
        ],
        [
         "26",
         "10",
         "79",
         "62",
         "9.0",
         "0"
        ],
        [
         "27",
         "3",
         "78",
         "61",
         "8.3",
         "0"
        ],
        [
         "28",
         "7",
         "57",
         "90",
         "4.8",
         "0"
        ],
        [
         "29",
         "4",
         "79",
         "64",
         "6.5",
         "0"
        ],
        [
         "30",
         "9",
         "40",
         "97",
         "8.4",
         "0"
        ],
        [
         "31",
         "3",
         "50",
         "100",
         "5.6",
         "0"
        ],
        [
         "32",
         "5",
         "67",
         "61",
         "7.9",
         "0"
        ],
        [
         "33",
         "3",
         "96",
         "97",
         "7.8",
         "0"
        ],
        [
         "34",
         "7",
         "64",
         "97",
         "6.2",
         "0"
        ],
        [
         "35",
         "5",
         "89",
         "61",
         "7.0",
         "0"
        ],
        [
         "36",
         "9",
         "62",
         "88",
         "4.5",
         "0"
        ],
        [
         "37",
         "7",
         "70",
         "91",
         "9.0",
         "0"
        ],
        [
         "38",
         "2",
         "69",
         "81",
         "6.6",
         "0"
        ],
        [
         "39",
         "4",
         "81",
         "45",
         "8.0",
         "0"
        ],
        [
         "40",
         "9",
         "74",
         "54",
         "7.7",
         "0"
        ],
        [
         "41",
         "2",
         "46",
         "93",
         "8.6",
         "0"
        ],
        [
         "42",
         "10",
         "99",
         "82",
         "6.5",
         "1"
        ],
        [
         "43",
         "9",
         "55",
         "99",
         "7.2",
         "1"
        ],
        [
         "44",
         "10",
         "65",
         "76",
         "7.7",
         "0"
        ],
        [
         "45",
         "5",
         "87",
         "72",
         "8.0",
         "0"
        ],
        [
         "46",
         "2",
         "96",
         "47",
         "8.4",
         "0"
        ],
        [
         "47",
         "4",
         "91",
         "92",
         "6.4",
         "0"
        ],
        [
         "48",
         "7",
         "99",
         "99",
         "6.2",
         "1"
        ],
        [
         "49",
         "8",
         "88",
         "83",
         "8.6",
         "0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 200
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyHours</th>\n",
       "      <th>Attendance</th>\n",
       "      <th>Assignments</th>\n",
       "      <th>SleepHours</th>\n",
       "      <th>Pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>61</td>\n",
       "      <td>65</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>74</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>87</td>\n",
       "      <td>89</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "      <td>64</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>72</td>\n",
       "      <td>63</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>8</td>\n",
       "      <td>87</td>\n",
       "      <td>65</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>47</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>4</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>65</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>6</td>\n",
       "      <td>94</td>\n",
       "      <td>49</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     StudyHours  Attendance  Assignments  SleepHours  Pass\n",
       "0             7          61           65         6.3     0\n",
       "1             4          50           74         6.6     0\n",
       "2             8          87           89         8.1     0\n",
       "3             5          55           64         8.7     0\n",
       "4             7          72           63         7.4     0\n",
       "..          ...         ...          ...         ...   ...\n",
       "195           8          87           65         7.8     0\n",
       "196           5          78           47         6.0     0\n",
       "197           4          68           68         7.0     0\n",
       "198           2          81           65         9.0     0\n",
       "199           6          94           49         8.2     0\n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)  # reproducibility\n",
    "\n",
    "# -----------------------------\n",
    "# Generate a \"feelable\" dataset\n",
    "# -----------------------------\n",
    "n = 200  # number of students (rows)\n",
    "\n",
    "# StudyHours: integers from 1 to 10 inclusive\n",
    "# np.random.randint(low, high, size) draws in [low, high)\n",
    "# so randint(1, 11, n) means min=1, max=10\n",
    "StudyHours = np.random.randint(1, 11, n)\n",
    "\n",
    "# Attendance: integers 40%..100% inclusive (min=40, max=100)\n",
    "Attendance = np.random.randint(40, 101, n)\n",
    "\n",
    "# Assignments: completion rate in percent 40..100 inclusive\n",
    "Assignments = np.random.randint(40, 101, n)\n",
    "\n",
    "# SleepHours: normal(mean=7, std=1.2), clipped to [4, 9] hours\n",
    "# The clip keeps values within the min/max bounds so we avoid unrealistic extremes\n",
    "SleepHours = np.random.normal(loc=7.0, scale=1.2, size=n).clip(4, 9)\n",
    "\n",
    "# -----------------------------\n",
    "# Rule-of-thumb \"true\" pattern\n",
    "# -----------------------------\n",
    "# We build a score that increases with StudyHours, Attendance, Assignments,\n",
    "# and slightly penalizes being far from ~7 hours of sleep.\n",
    "score = (\n",
    "    0.4 * StudyHours                 # study helps a lot\n",
    "    + 0.3 * (Attendance / 10.0)      # scale % to 0..10-ish range\n",
    "    + 0.2 * (Assignments / 10.0)     # same idea as Attendance\n",
    "    - 0.8 * np.abs(SleepHours - 7.0) # penalty for too little or too much sleep\n",
    ")\n",
    "\n",
    "# Convert score -> pass probability through a logistic link\n",
    "# Steeper slope (0.8) makes the pass/fail boundary clearer\n",
    "# \n",
    "# Why \"logistic\" function?\n",
    "#   - Originally derived from the logistic equation used to model population growth\n",
    "#   - Creates an S-shaped (sigmoid) curve that smoothly transitions from 0 to 1\n",
    "#   - Perfect for converting continuous scores into probabilities\n",
    "#\n",
    "# Logistic function breakdown:\n",
    "#   - We shift the score by subtracting 7.0 (centers the decision boundary at score=7)\n",
    "#   - Multiply by -0.8 to control the steepness (higher = sharper transition)\n",
    "#   - np.exp(-0.8 * (score - 7.0)) computes e^(-0.8*(score-7))\n",
    "#   - Add 1.0 to the exponential result\n",
    "#   - Take the reciprocal (1.0 / ...) to get the sigmoid/logistic curve\n",
    "#   - Result: probability in range [0, 1] where score=7 maps to prob≈0.5\n",
    "prob = 1.0 / (1.0 + np.exp(-0.8 * (score - 7.0)))\n",
    "\n",
    "# print(prob)\n",
    "\n",
    "# Final label: Pass=1 if prob>0.5 else 0\n",
    "Pass = (prob > 0.5).astype(int)\n",
    "\n",
    "# Assemble the DataFrame (round sleep for nicer printing)\n",
    "df = pd.DataFrame({\n",
    "    \"StudyHours\": StudyHours,\n",
    "    \"Attendance\": Attendance,\n",
    "    \"Assignments\": Assignments,\n",
    "    \"SleepHours\": np.round(SleepHours, 1),\n",
    "    \"Pass\": Pass\n",
    "})\n",
    "\n",
    "\n",
    "df.head(200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f301f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "F1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "3e9b5a51-4195-46ed-9715-fc8bd01b8b9c",
       "rows": [
        [
         "Logistic Regression",
         "0.96",
         "0.75"
        ],
        [
         "Decision Tree",
         "0.94",
         "0.6666666666666666"
        ],
        [
         "Random Forest",
         "0.96",
         "0.75"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy        F1\n",
       "Logistic Regression      0.96  0.750000\n",
       "Decision Tree            0.94  0.666667\n",
       "Random Forest            0.96  0.750000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Train/Test split\n",
    "# -----------------------------\n",
    "X = df[[\"StudyHours\", \"Attendance\", \"Assignments\", \"SleepHours\"]]\n",
    "y = df[\"Pass\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Why scale data? Example: StudyHours ranges 1-10, but Attendance ranges 40-100.\n",
    "# Without scaling, Attendance (larger numbers) would dominate distance-based models like Logistic Regression.\n",
    "# Standardize features for models that benefit from scaling (e.g., Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Why fit_transform vs. transform?\n",
    "#   - fit_transform(): learns the mean & std from X_train, then scales X_train using those values\n",
    "#   - transform(): applies the SAME mean & std (learned from training) to X_test\n",
    "#   - This prevents data leakage — test data must never influence the scaling parameters\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# -----------------------------\n",
    "# Baseline models\n",
    "# -----------------------------\n",
    "# 1) Logistic Regression (needs scaling)\n",
    "log_reg = LogisticRegression(max_iter=500, random_state=42)\n",
    "\n",
    "# 2) Decision Tree (intuitive, can overfit; scaling not required but harmless if used)\n",
    "tree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "\n",
    "# 3) Random Forest (built-in bagging of trees)\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "# Fit/evaluate baselines\n",
    "results = {}\n",
    "\n",
    "# Logistic Regression on scaled data\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "pred_lr = log_reg.predict(X_test_scaled)\n",
    "results[\"Logistic Regression\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_test, pred_lr),\n",
    "    \"F1\": f1_score(y_test, pred_lr)\n",
    "}\n",
    "\n",
    "# Decision Tree on *unscaled* numeric features (trees are scale-invariant)\n",
    "tree.fit(X_train, y_train)\n",
    "pred_tree = tree.predict(X_test)\n",
    "results[\"Decision Tree\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_test, pred_tree),\n",
    "    \"F1\": f1_score(y_test, pred_tree)\n",
    "}\n",
    "\n",
    "# Random Forest on *unscaled* features (tree-based, also scale-invariant)\n",
    "rf.fit(X_train, y_train)\n",
    "pred_rf = rf.predict(X_test)\n",
    "results[\"Random Forest\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_test, pred_rf),\n",
    "    \"F1\": f1_score(y_test, pred_rf)\n",
    "}\n",
    "\n",
    "pd.DataFrame(results).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17274bd3",
   "metadata": {},
   "source": [
    "### Interpreting the Baseline Results\n",
    "\n",
    "**How to read these numbers**\n",
    "\n",
    "- **Accuracy** tells us the overall proportion of correct predictions.  \n",
    "  Here, all three models perform quite well — around 94–96%, meaning they correctly classify most students as Pass or Fail.\n",
    "\n",
    "- **F1 Score** balances *precision* and *recall*, so it’s more reliable when one class (like “Pass”) is more common than the other.  \n",
    "  It ranges from 0 to 1, where higher means better balance between detecting both classes.\n",
    "\n",
    "---\n",
    "\n",
    "**What we can see:**\n",
    "\n",
    "- **Logistic Regression** and **Random Forest** perform equally well (F1 = 0.75).  \n",
    "\n",
    "  \n",
    "- **Decision Tree** alone performs slightly worse (F1 = 0.67), likely because a single tree can overfit to the training data and lose generalization.\n",
    "\n",
    "- The **Random Forest**, which averages many trees, fixes much of that instability — matching the Logistic Regression’s accuracy and F1.  \n",
    "  This already shows the benefit of an ensemble: better balance and more consistent performance.\n",
    "\n",
    "---\n",
    "\n",
    "**Takeaway:**  \n",
    "Even at this baseline stage, the ensemble model (Random Forest) is more reliable than a single Decision Tree.  \n",
    "Next, we’ll introduce **Bagging**, **Boosting**, and **Voting** to see whether combining models can push this performance even higher or make it more stable across different samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "213827ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "F1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "cc216a49-fe08-49fb-8ef9-693d48b60515",
       "rows": [
        [
         "AdaBoost",
         "0.96",
         "0.8"
        ],
        [
         "Logistic Regression",
         "0.96",
         "0.75"
        ],
        [
         "Bagging (Tree)",
         "0.96",
         "0.75"
        ],
        [
         "Random Forest",
         "0.96",
         "0.75"
        ],
        [
         "Decision Tree",
         "0.94",
         "0.6666666666666666"
        ],
        [
         "Soft Voting (LR+Tree+RF)",
         "0.94",
         "0.5714285714285714"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging (Tree)</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soft Voting (LR+Tree+RF)</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Accuracy        F1\n",
       "AdaBoost                      0.96  0.800000\n",
       "Logistic Regression           0.96  0.750000\n",
       "Bagging (Tree)                0.96  0.750000\n",
       "Random Forest                 0.96  0.750000\n",
       "Decision Tree                 0.94  0.666667\n",
       "Soft Voting (LR+Tree+RF)      0.94  0.571429"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# ENSEMBLE METHODS OVERVIEW\n",
    "# -----------------------------\n",
    "# \n",
    "# BAGGING (Bootstrap Aggregating):\n",
    "#   - Trains multiple models in PARALLEL on different random subsets of data (with replacement)\n",
    "#   - Each model votes equally, and we average their predictions\n",
    "#   - Goal: Reduce VARIANCE (prevents overfitting by smoothing out individual model quirks)\n",
    "#   - Example: Random Forest is bagging with decision trees\n",
    "#\n",
    "# BOOSTING (e.g., AdaBoost):\n",
    "#   - Trains models SEQUENTIALLY, where each new model focuses on fixing previous mistakes\n",
    "#   - Later models get more weight if they perform better\n",
    "#   - Goal: Reduce BIAS (turns weak learners into a strong ensemble)\n",
    "#   - Pays extra attention to hard-to-classify examples\n",
    "#\n",
    "# Key difference: Bagging = parallel + equal weight | Boosting = sequential + weighted focus on errors\n",
    "\n",
    "# -----------------------------\n",
    "# Bagging: many trees in parallel, average their votes\n",
    "# -----------------------------\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    n_estimators=150,\n",
    "    random_state=42\n",
    ")\n",
    "bagging.fit(X_train, y_train)  # tree-based → no scaling required\n",
    "pred_bag = bagging.predict(X_test)\n",
    "results[\"Bagging (Tree)\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_test, pred_bag),\n",
    "    \"F1\": f1_score(y_test, pred_bag)\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# AdaBoost: sequentially fix mistakes (weak trees → strong ensemble)\n",
    "# -----------------------------\n",
    "boost = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=2, random_state=42),  # shallow stumps\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "boost.fit(X_train, y_train)  # tree-based → no scaling required\n",
    "pred_boost = boost.predict(X_test)\n",
    "results[\"AdaBoost\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_test, pred_boost),\n",
    "    \"F1\": f1_score(y_test, pred_boost)\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Soft Voting: combine different *types* of learners\n",
    "# (LogReg uses scaled features; trees use raw features)\n",
    "# To make this clean, we re-fit internal copies consistently.\n",
    "# -----------------------------\n",
    "lr_for_vote   = LogisticRegression(max_iter=500, random_state=42)\n",
    "tree_for_vote = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "rf_for_vote   = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "# Fit each on its appropriate representation\n",
    "lr_for_vote.fit(X_train_scaled, y_train)\n",
    "tree_for_vote.fit(X_train, y_train)\n",
    "rf_for_vote.fit(X_train, y_train)\n",
    "\n",
    "# Soft-vote average of probabilities (equal weights)\n",
    "proba_lr   = lr_for_vote.predict_proba(X_test_scaled)\n",
    "proba_tree = tree_for_vote.predict_proba(X_test)\n",
    "proba_rf   = rf_for_vote.predict_proba(X_test)\n",
    "\n",
    "proba_soft = (proba_lr + proba_tree + proba_rf) / 3.0\n",
    "pred_vote  = (proba_soft[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "results[\"Soft Voting (LR+Tree+RF)\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_test, pred_vote),\n",
    "    \"F1\": f1_score(y_test, pred_vote)\n",
    "}\n",
    "\n",
    "# Final comparison table\n",
    "comparison = pd.DataFrame(results).T.sort_values(\"F1\", ascending=False)\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06986057",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### 1. Reading the overall pattern\n",
    "All models perform well in terms of **Accuracy** (94–96%), meaning almost all students are correctly classified as Pass or Fail.  \n",
    "However, **F1 Score** gives us a better sense of *balance* between correctly identifying both classes — and that’s where we see the difference.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. AdaBoost stands out\n",
    "- **AdaBoost (F1 = 0.80)** clearly outperforms all others.  \n",
    "- It does this by **sequentially focusing on the harder-to-predict cases**, improving precision and recall for the minority class (likely “Fail”).  \n",
    "- In plain terms, AdaBoost doesn’t just get most students right — it gets the tricky ones right more often.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Bagging and Random Forest stabilize\n",
    "- **Bagging (Tree)** and **Random Forest** both hit the same solid mark as **Logistic Regression** (F1 = 0.75).  \n",
    "- That means they’re **consistent** and **less sensitive** to training noise compared to a single tree, but they don’t surpass AdaBoost because the dataset is already simple and relatively clean.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. The Decision Tree lags slightly\n",
    "- The **Decision Tree (F1 = 0.67)** underperforms because a single tree can **overfit** the training data.  \n",
    "- Ensembles like Random Forest or Bagging fix that by averaging many trees — reducing variance.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. The Soft Voting dip\n",
    "- Interestingly, **Soft Voting (F1 = 0.57)** performs worse here.  \n",
    "- Why? Because it combined models that all made *similar types of errors*.  \n",
    "- When base models are not diverse enough, averaging their probabilities can actually **dilute** confident correct predictions, reducing recall for the minority class.\n",
    "\n",
    "---\n",
    "\n",
    "### Takeaway\n",
    "\n",
    "- **AdaBoost** emerges as the most effective ensemble for this dataset.  \n",
    "  It demonstrates the key advantage of boosting: **higher precision–recall balance by correcting prior mistakes.**\n",
    "- **Bagging** and **Random Forest** show why averaging improves stability — they’re reliable and never perform poorly.\n",
    "- **Voting** shows that combining models only helps when those models truly **complement** one another.\n",
    "\n",
    "In practice, ensemble learning isn’t about “one-size-fits-all.”  \n",
    "You pick the right combination depending on whether your goal is **accuracy, stability, or sensitivity to rare cases** — and AdaBoost nailed that balance here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f09a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression ===\n",
      "Accuracy: 0.96\n",
      "F1      : 0.75\n",
      "Confusion Matrix:\n",
      " [[45  0]\n",
      " [ 2  3]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.957     1.000     0.978        45\n",
      "           1      1.000     0.600     0.750         5\n",
      "\n",
      "    accuracy                          0.960        50\n",
      "   macro avg      0.979     0.800     0.864        50\n",
      "weighted avg      0.962     0.960     0.955        50\n",
      "\n",
      "\n",
      "=== Decision Tree ===\n",
      "Accuracy: 0.94\n",
      "F1      : 0.667\n",
      "Confusion Matrix:\n",
      " [[44  1]\n",
      " [ 2  3]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.957     0.978     0.967        45\n",
      "           1      0.750     0.600     0.667         5\n",
      "\n",
      "    accuracy                          0.940        50\n",
      "   macro avg      0.853     0.789     0.817        50\n",
      "weighted avg      0.936     0.940     0.937        50\n",
      "\n",
      "\n",
      "=== Random Forest ===\n",
      "Accuracy: 0.96\n",
      "F1      : 0.75\n",
      "Confusion Matrix:\n",
      " [[45  0]\n",
      " [ 2  3]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.957     1.000     0.978        45\n",
      "           1      1.000     0.600     0.750         5\n",
      "\n",
      "    accuracy                          0.960        50\n",
      "   macro avg      0.979     0.800     0.864        50\n",
      "weighted avg      0.962     0.960     0.955        50\n",
      "\n",
      "\n",
      "=== Bagging (Tree) ===\n",
      "Accuracy: 0.96\n",
      "F1      : 0.75\n",
      "Confusion Matrix:\n",
      " [[45  0]\n",
      " [ 2  3]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.957     1.000     0.978        45\n",
      "           1      1.000     0.600     0.750         5\n",
      "\n",
      "    accuracy                          0.960        50\n",
      "   macro avg      0.979     0.800     0.864        50\n",
      "weighted avg      0.962     0.960     0.955        50\n",
      "\n",
      "\n",
      "=== AdaBoost ===\n",
      "Accuracy: 0.96\n",
      "F1      : 0.8\n",
      "Confusion Matrix:\n",
      " [[44  1]\n",
      " [ 1  4]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.978     0.978     0.978        45\n",
      "           1      0.800     0.800     0.800         5\n",
      "\n",
      "    accuracy                          0.960        50\n",
      "   macro avg      0.889     0.889     0.889        50\n",
      "weighted avg      0.960     0.960     0.960        50\n",
      "\n",
      "\n",
      "=== Soft Voting (LR+Tree+RF) ===\n",
      "Accuracy: 0.94\n",
      "F1      : 0.571\n",
      "Confusion Matrix:\n",
      " [[45  0]\n",
      " [ 3  2]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.938     1.000     0.968        45\n",
      "           1      1.000     0.400     0.571         5\n",
      "\n",
      "    accuracy                          0.940        50\n",
      "   macro avg      0.969     0.700     0.770        50\n",
      "weighted avg      0.944     0.940     0.928        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def show_report(name, y_true, y_pred):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 3))\n",
    "    print(\"F1      :\", round(f1_score(y_true, y_pred), 3))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "show_report(\"Logistic Regression\", y_test, pred_lr)\n",
    "show_report(\"Decision Tree\", y_test, pred_tree)\n",
    "show_report(\"Random Forest\", y_test, pred_rf)\n",
    "show_report(\"Bagging (Tree)\", y_test, pred_bag)\n",
    "show_report(\"AdaBoost\", y_test, pred_boost)\n",
    "show_report(\"Soft Voting (LR+Tree+RF)\", y_test, pred_vote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02d64d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.barh(comparison.index, comparison[\"F1\"], alpha=0.8)\n",
    "plt.xlabel(\"F1 Score\")\n",
    "plt.title(\"Model Comparison (Higher is Better)\")\n",
    "plt.grid(axis=\"x\", alpha=0.35)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962fc910",
   "metadata": {},
   "source": [
    "\n",
    "### What improved, and why it matters\n",
    "\n",
    "- **Decision Tree → Bagging/Random Forest:**  \n",
    "  A single tree can overfit to specific quirks of the training set.  \n",
    "  By **averaging many trees** trained on different bootstrap samples, Bagging/Random Forest **reduce variance**,  \n",
    "  which typically raises F1 and Accuracy on the test set.  \n",
    "  In our results table, compare “Decision Tree” vs “Bagging (Tree)” and “Random Forest” to see this stabilization.\n",
    "\n",
    "- **AdaBoost vs. single tree:**  \n",
    "  AdaBoost builds trees **sequentially**, each one focusing on correcting the previous mistakes.  \n",
    "  This **reduces bias** and often pushes performance higher than Bagging on clean tabular data.  \n",
    "  If its F1 is on top or near the top, that’s AdaBoost doing its job: turning simple trees into a strong learner.\n",
    "\n",
    "- **Soft Voting (LR + Tree + RF):**  \n",
    "  Different models make **different mistakes**.  \n",
    "  By averaging their predicted probabilities (soft voting), we **smooth out idiosyncratic errors** and gain robustness.  \n",
    "  If the voting F1 lands among the best, that tells us the learners are **complementary**.\n",
    "\n",
    "- **Why F1 (not just Accuracy):**  \n",
    "  F1 balances **precision** and **recall**, which is helpful if the pass/fail classes are somewhat imbalanced.  \n",
    "  A higher F1 means **fewer false alarms and fewer misses** overall.\n",
    "\n",
    "**Bottom line:**  \n",
    "On this student-style dataset, ensembles (Bagging/Random Forest, AdaBoost, or Voting) **consistently outperform** or at least **stabilize** single models.  \n",
    "This is exactly why ensembles are a go-to tactic in real projects: quick gains with minimal tuning, and performance that’s less fragile.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "william-ml-3-12-12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
